{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport string\nimport numpy as np\nimport random\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nfrom IPython.core.debugger import set_trace","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-30T15:48:52.138716Z","iopub.execute_input":"2024-03-30T15:48:52.139179Z","iopub.status.idle":"2024-03-30T15:48:52.146577Z","shell.execute_reply.started":"2024-03-30T15:48:52.139143Z","shell.execute_reply":"2024-03-30T15:48:52.145342Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def get_alphabets_random(window, shift, text=None, reverse=False):\n    if text is not None:\n        shift = shift % len(text)\n        shifted_text = text[shift:] + text[:shift]\n        return f'Input: {text}\\nShifted: {shifted_text}'\n    \n    alphabets = string.ascii_lowercase\n    rand_indices = [random.randint(0, 25) for _ in range(window)]\n    \n    text = ''.join(list(map(lambda x: alphabets[x], rand_indices)))\n    \n    if reverse:\n        reversed_text = ''.join(list(reversed(list(text))))\n        \n    shift = shift % len(text)\n    shifted_text = text[shift:] + text[:shift]\n    \n    return text, shifted_text\n\nget_alphabets_random(10, -2, reverse=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:48:52.149771Z","iopub.execute_input":"2024-03-30T15:48:52.150327Z","iopub.status.idle":"2024-03-30T15:48:52.163152Z","shell.execute_reply.started":"2024-03-30T15:48:52.150285Z","shell.execute_reply":"2024-03-30T15:48:52.162037Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"('srjfhinncw', 'cwsrjfhinn')"},"metadata":{}}]},{"cell_type":"code","source":"def generate_training_data(window, shift, num_examples):\n    x = list(); y = list()\n    \n    for _ in range(num_examples):\n        text, shifted = get_alphabets_random(window, shift, reverse=False)\n        x.append(text); y.append(shifted)\n        \n    random_id = random.randint(0, len(x))\n    print(x[random_id], y[random_id])\n    \n    return x, y","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:48:52.164885Z","iopub.execute_input":"2024-03-30T15:48:52.165225Z","iopub.status.idle":"2024-03-30T15:48:52.176019Z","shell.execute_reply.started":"2024-03-30T15:48:52.165182Z","shell.execute_reply":"2024-03-30T15:48:52.175119Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def create_vocabulary():\n    unique_characters = list(string.ascii_lowercase)\n    char_to_id = {char:i for i, char in enumerate(unique_characters)}\n    id_to_char = {i: char for i, char in enumerate(unique_characters)}\n    \n    features_dim = len(unique_characters)\n    print(f'Number of features is {features_dim}')\n    return char_to_id, id_to_char, features_dim","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:48:52.177579Z","iopub.execute_input":"2024-03-30T15:48:52.177899Z","iopub.status.idle":"2024-03-30T15:48:52.185917Z","shell.execute_reply.started":"2024-03-30T15:48:52.177870Z","shell.execute_reply":"2024-03-30T15:48:52.184976Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"encode_text = lambda x: [char_to_id[_] for _ in x]\ndecode_text = lambda x_: [id_to_char[_] for _ in x_]","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:48:52.188010Z","iopub.execute_input":"2024-03-30T15:48:52.188451Z","iopub.status.idle":"2024-03-30T15:48:52.196652Z","shell.execute_reply.started":"2024-03-30T15:48:52.188393Z","shell.execute_reply":"2024-03-30T15:48:52.195490Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def encode_data(x, y, features_dim):\n    encoded_x = list(map(lambda x_: encode_text(x_), x))\n    encoded_y = list(map(lambda x_: encode_text(x_), y))\n    \n    one_hot_x = F.one_hot(torch.tensor(encoded_x), num_classes=features_dim)\n    one_hot_y = F.one_hot(torch.tensor(encoded_y), num_classes=features_dim)\n    \n    one_hot_y = one_hot_y.permute(1, 0, 2)\n    \n    return one_hot_x.to(torch.float32), one_hot_y.to(torch.float32)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:48:52.202519Z","iopub.execute_input":"2024-03-30T15:48:52.202930Z","iopub.status.idle":"2024-03-30T15:48:52.211266Z","shell.execute_reply.started":"2024-03-30T15:48:52.202888Z","shell.execute_reply":"2024-03-30T15:48:52.210033Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"x, y = generate_training_data(10, -2, 10000)\nchar_to_id, id_to_char, features = create_vocabulary()\none_hot_x, one_hot_y = encode_data(x, y, features)\nprint(one_hot_x.dtype, one_hot_y.dtype)\n\nval_x, val_y = generate_training_data(10, -2, 1000)\nval_x, val_y = encode_data(val_x, val_y, features)\nprint(val_x.shape, val_y.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:48:52.213236Z","iopub.execute_input":"2024-03-30T15:48:52.213556Z","iopub.status.idle":"2024-03-30T15:48:52.834564Z","shell.execute_reply.started":"2024-03-30T15:48:52.213529Z","shell.execute_reply":"2024-03-30T15:48:52.833408Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"spqkmisviz izspqkmisv\nNumber of features is 26\ntorch.float32 torch.float32\nhowmttxxgx gxhowmttxx\ntorch.Size([1000, 10, 26]) torch.Size([10, 1000, 26])\n","output_type":"stream"}]},{"cell_type":"code","source":"decode_text(torch.argmax(one_hot_x[0], dim=-1).tolist()), decode_text(torch.argmax(one_hot_y[:, 0, :], dim=-1).tolist())","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:48:52.835909Z","iopub.execute_input":"2024-03-30T15:48:52.836454Z","iopub.status.idle":"2024-03-30T15:48:52.845170Z","shell.execute_reply.started":"2024-03-30T15:48:52.836418Z","shell.execute_reply":"2024-03-30T15:48:52.844055Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"(['r', 'l', 'w', 'e', 'a', 'c', 'w', 'f', 's', 'y'],\n ['s', 'y', 'r', 'l', 'w', 'e', 'a', 'c', 'w', 'f'])"},"metadata":{}}]},{"cell_type":"code","source":"class CustomData(Dataset):\n    def __init__(self, x_data, y_data):\n        super().__init__()\n        self.x_data = x_data\n        self.y_data = y_data\n        \n    def __len__(self):\n        return len(self.x_data)\n    \n    def __getitem__(self, idx):\n        return self.x_data[idx], self.y_data[:, idx, :]\n    \ndataset = CustomData(one_hot_x, one_hot_y)\nval_dataset = CustomData(val_x, val_y)\n\ntrain_loader = DataLoader(dataset, batch_size=128, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=128, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:48:52.847705Z","iopub.execute_input":"2024-03-30T15:48:52.848102Z","iopub.status.idle":"2024-03-30T15:48:52.859150Z","shell.execute_reply.started":"2024-03-30T15:48:52.848070Z","shell.execute_reply":"2024-03-30T15:48:52.857699Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"class AttentionModel(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim, input_features_dim, output_features_dim, timesteps, num_training):\n        super().__init__()\n        self.timesteps = timesteps\n        self.input_features_dim = input_features_dim\n        \n        self.densor1 = nn.Linear(decoder_dim+(2*encoder_dim), 10)\n        self.tanh = nn.Tanh()\n        self.densor2 = nn.Linear(10, 1)\n        self.softmax = nn.Softmax(dim=1)\n        \n        self.pre_attention_lstm = nn.LSTM(input_size=input_features_dim, hidden_size=encoder_dim, batch_first=True, bidirectional=True)\n        self.post_attention_lstm = nn.LSTM(input_size=encoder_dim*2, hidden_size=decoder_dim, batch_first=True)\n        self.output_layer = nn.Linear(decoder_dim, output_features_dim)\n        \n        \n    def forward(self, x, s0, c0):\n        # x-> (B, timesteps, features) -> (128, 10, 26)\n        # s0, c0 -> (1, B, decoder_dim) -> (1, 128, 16)\n        #set_trace()\n        self.s = s0; self.c = c0\n        encoder_hidden_states, (hn, cn) = self.pre_attention_lstm(x) #hidden_states: (B, timesteps, encoder_dim*2) -> (128, 10, 16)\n        outputs = list()\n        \n        for t in range(self.timesteps):\n            context = self.one_step_attention(encoder_hidden_states) # (B, timesteps, 2*encoder_dim)\n            _, (self.s, self.c )= self.post_attention_lstm(context, (self.s, self.c)) # _ -> (B, timesteps, decoder_dim)\n            output = self.output_layer(self.s) # (1, B, output_features_dim) -> (1, 128, 26)\n            outputs.append(output) # (timesteps, B, output_features_dim) -> (10, 128, 26)\n        \n        return outputs\n    \n    def one_step_attention(self, encoder_hidden_states):\n        # encoder_Hidden_states -> (B, timesteps, 2*encoder_dim) -> (128, 10, 16)\n        # self.s -> (1, B, decoder_dim)\n        # 1. first the hidden state for decoder must be repeated to match the hidden states of encoder\n        # self.s -> (timesteps, B, decoder_dim) -> permute -> (B, timesteps, decoder_dim)\n        # 2. then concatenate the hidden state for decoder and the hidden state for encoder -> (B, timesteps, 2*encoder_dim + decoder_dim)\n        # pass it through the first dense layer\n        # pass it through the second dense layer\n        # use softmax to decide which hidden state of encoder is the most important\n        # use dot product to find the important hidden state of encoder and feed it as input to the decoder\n        hidden_decoder = self.s.repeat(10, 1, 1).permute(1, 0, 2) # (B, timesteps, decoder_dim) -> (128, 10, 16)\n        concat = torch.concatenate([encoder_hidden_states, hidden_decoder], dim=-1) # (B, timesteps, 2*encoder_dim + decoder_dim) -> (128, 10, 32)\n        e = self.tanh(self.densor1(concat)) # (B, timesteps, 10) -> (128, 10, 10)\n        energies = self.softmax(self.densor2(e)) # (B, timesteps, 1) -> (128, 10, 1)\n        # let's if without permute in next step if the code converges\n        energies = energies.repeat(1, 1, 10).permute(0, 2, 1) # (B, timesteps, 10) -> (B, 10, 10)\n        context = torch.bmm(energies, encoder_hidden_states) # (B, timesteps, timesteps) @ (B, timesteps, 2*encoder_dim) -> (B, timesteps, 2*encoder_dim)\n        \n        return context\n        \n    def predict(self, x):\n        assert len(x) == self.timesteps\n        encoded = encode_text(x)\n        one_hot = F.one_hot(torch.tensor(encoded), num_classes=self.input_features_dim)\n        s0 = torch.zeros(1, 1, decoder_dim); c0 = torch.zeros(1, 1, decoder_dim)\n        one_hot = one_hot.unsqueeze(0)\n        pred = self.forward(one_hot.to(torch.float32), s0, c0)\n        pred = ''.join(decode_text([torch.argmax(t, dim=-1).tolist()[0][0] for t in pred]))\n        return pred","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:48:52.860842Z","iopub.execute_input":"2024-03-30T15:48:52.861374Z","iopub.status.idle":"2024-03-30T15:48:52.882155Z","shell.execute_reply.started":"2024-03-30T15:48:52.861340Z","shell.execute_reply":"2024-03-30T15:48:52.881000Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def calculate_loss(y_true, y_pred):\n    ce = nn.CrossEntropyLoss()\n    total = 0\n    for target, logit in zip(list(y_true), list(y_pred)):\n        loss = ce(logit[0], target)\n        total += loss\n        \n    return total","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:48:52.883661Z","iopub.execute_input":"2024-03-30T15:48:52.884041Z","iopub.status.idle":"2024-03-30T15:48:52.896651Z","shell.execute_reply.started":"2024-03-30T15:48:52.884009Z","shell.execute_reply":"2024-03-30T15:48:52.894868Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"encoder_dim = 8\ndecoder_dim = 16\ninput_features_dim = features\noutput_features_dim = features\ntimesteps = 10\nnum_training = 10000\nepochs = 40\n\ns0, c0 = torch.zeros(1, num_training, decoder_dim), torch.zeros(1, num_training, decoder_dim)\n\nam = AttentionModel(encoder_dim, decoder_dim, input_features_dim, output_features_dim, timesteps, num_training)\nopt = optim.Adam(am.parameters())\n\nfor epoch in range(epochs):\n    am.train()\n    train_loss = list()\n    for i, (inputs, target) in enumerate(train_loader):\n        #set_trace()\n        target = target.permute(1, 0, 2)\n        batch_size = inputs.size(0)\n        s0 = torch.zeros(1, batch_size, decoder_dim); c0 = torch.zeros(1, batch_size, decoder_dim)\n        outputs = am(inputs, s0, c0)\n        \n        opt.zero_grad()\n        \n        total_loss = calculate_loss(target, outputs)\n        train_loss.append(total_loss)\n        \n        total_loss.backward()\n        opt.step()\n        \n    print(f'Epoch {epoch}:: Train Loss {torch.mean(torch.tensor(train_loss))}')\n    \n    am.eval()\n    with torch.no_grad():\n        s0 = torch.zeros(1, 1000, decoder_dim); c0 = torch.zeros(1, 1000, decoder_dim)\n        out = am(val_x, s0, c0)\n        \n        val_loss = calculate_loss(val_y, out)\n        \n        print(f'Epoch {epoch}:: Val Loss {val_loss}')\n        test = 'monojitabc'\n        pred = am.predict(test)\n        print(f'Input {test} --> Output {pred}')\n        print()\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:48:52.899095Z","iopub.execute_input":"2024-03-30T15:48:52.899943Z","iopub.status.idle":"2024-03-30T15:51:02.040760Z","shell.execute_reply.started":"2024-03-30T15:48:52.899855Z","shell.execute_reply":"2024-03-30T15:51:02.039351Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Epoch 0:: Train Loss 32.58932113647461\nEpoch 0:: Val Loss 32.409278869628906\nInput monojitabc --> Output dddddddddd\n\nEpoch 1:: Train Loss 31.928136825561523\nEpoch 1:: Val Loss 31.520450592041016\nInput monojitabc --> Output oooooooooo\n\nEpoch 2:: Train Loss 31.12347984313965\nEpoch 2:: Val Loss 30.870098114013672\nInput monojitabc --> Output oooooooooo\n\nEpoch 3:: Train Loss 30.585054397583008\nEpoch 3:: Val Loss 30.414852142333984\nInput monojitabc --> Output oooooooooo\n\nEpoch 4:: Train Loss 30.222251892089844\nEpoch 4:: Val Loss 30.066448211669922\nInput monojitabc --> Output oooooooooo\n\nEpoch 5:: Train Loss 29.84393882751465\nEpoch 5:: Val Loss 29.64544677734375\nInput monojitabc --> Output oooooooooo\n\nEpoch 6:: Train Loss 29.44198989868164\nEpoch 6:: Val Loss 29.326677322387695\nInput monojitabc --> Output oooooooooo\n\nEpoch 7:: Train Loss 29.112918853759766\nEpoch 7:: Val Loss 28.930233001708984\nInput monojitabc --> Output aooooooooo\n\nEpoch 8:: Train Loss 28.649394989013672\nEpoch 8:: Val Loss 28.372169494628906\nInput monojitabc --> Output aooooooooo\n\nEpoch 9:: Train Loss 27.730039596557617\nEpoch 9:: Val Loss 26.80229949951172\nInput monojitabc --> Output looooooooo\n\nEpoch 10:: Train Loss 25.72160530090332\nEpoch 10:: Val Loss 24.654983520507812\nInput monojitabc --> Output cooopppppo\n\nEpoch 11:: Train Loss 22.976459503173828\nEpoch 11:: Val Loss 21.306407928466797\nInput monojitabc --> Output ccmooppitt\n\nEpoch 12:: Train Loss 19.20549201965332\nEpoch 12:: Val Loss 17.751691818237305\nInput monojitabc --> Output scoooobitt\n\nEpoch 13:: Train Loss 15.351587295532227\nEpoch 13:: Val Loss 13.442665100097656\nInput monojitabc --> Output scmooojitt\n\nEpoch 14:: Train Loss 11.927327156066895\nEpoch 14:: Val Loss 10.344122886657715\nInput monojitabc --> Output scmonojitt\n\nEpoch 15:: Train Loss 8.944119453430176\nEpoch 15:: Val Loss 7.795470714569092\nInput monojitabc --> Output scmonojitt\n\nEpoch 16:: Train Loss 6.742868900299072\nEpoch 16:: Val Loss 5.969646453857422\nInput monojitabc --> Output scmonojitt\n\nEpoch 17:: Train Loss 5.2328643798828125\nEpoch 17:: Val Loss 4.561748027801514\nInput monojitabc --> Output scmonojita\n\nEpoch 18:: Train Loss 4.0650811195373535\nEpoch 18:: Val Loss 3.6611874103546143\nInput monojitabc --> Output scmonojita\n\nEpoch 19:: Train Loss 3.337782382965088\nEpoch 19:: Val Loss 3.2543396949768066\nInput monojitabc --> Output scmonojita\n\nEpoch 20:: Train Loss 2.7655868530273438\nEpoch 20:: Val Loss 2.4754629135131836\nInput monojitabc --> Output scmonojita\n\nEpoch 21:: Train Loss 2.3195152282714844\nEpoch 21:: Val Loss 2.1809847354888916\nInput monojitabc --> Output scmonojita\n\nEpoch 22:: Train Loss 1.983113408088684\nEpoch 22:: Val Loss 1.8692467212677002\nInput monojitabc --> Output bcmonojita\n\nEpoch 23:: Train Loss 1.6177171468734741\nEpoch 23:: Val Loss 1.4919246435165405\nInput monojitabc --> Output bcmonojita\n\nEpoch 24:: Train Loss 1.2964470386505127\nEpoch 24:: Val Loss 1.1146914958953857\nInput monojitabc --> Output bcmonojita\n\nEpoch 25:: Train Loss 0.9836474061012268\nEpoch 25:: Val Loss 0.8758689165115356\nInput monojitabc --> Output bcmonojita\n\nEpoch 26:: Train Loss 0.8675255179405212\nEpoch 26:: Val Loss 0.7378746867179871\nInput monojitabc --> Output bcmonojita\n\nEpoch 27:: Train Loss 0.6531585454940796\nEpoch 27:: Val Loss 0.5770338177680969\nInput monojitabc --> Output bcmonojita\n\nEpoch 28:: Train Loss 0.5303150415420532\nEpoch 28:: Val Loss 0.47383400797843933\nInput monojitabc --> Output bcmonojita\n\nEpoch 29:: Train Loss 0.4417122006416321\nEpoch 29:: Val Loss 0.40119439363479614\nInput monojitabc --> Output bcmonojita\n\nEpoch 30:: Train Loss 0.3771522641181946\nEpoch 30:: Val Loss 0.34633052349090576\nInput monojitabc --> Output bcmonojita\n\nEpoch 31:: Train Loss 0.32125619053840637\nEpoch 31:: Val Loss 0.3011694848537445\nInput monojitabc --> Output bcmonojita\n\nEpoch 32:: Train Loss 0.2926623523235321\nEpoch 32:: Val Loss 0.30407536029815674\nInput monojitabc --> Output bcmonojita\n\nEpoch 33:: Train Loss 0.2697305679321289\nEpoch 33:: Val Loss 0.236918643116951\nInput monojitabc --> Output bcmonojita\n\nEpoch 34:: Train Loss 0.23606587946414948\nEpoch 34:: Val Loss 0.2151450514793396\nInput monojitabc --> Output bcmonojita\n\nEpoch 35:: Train Loss 0.23531566560268402\nEpoch 35:: Val Loss 8.19845199584961\nInput monojitabc --> Output bcmonooita\n\nEpoch 36:: Train Loss 0.6663654446601868\nEpoch 36:: Val Loss 0.24310168623924255\nInput monojitabc --> Output bcmonojita\n\nEpoch 37:: Train Loss 0.2147550731897354\nEpoch 37:: Val Loss 0.18525464832782745\nInput monojitabc --> Output bcmonojita\n\nEpoch 38:: Train Loss 0.17657415568828583\nEpoch 38:: Val Loss 0.16026091575622559\nInput monojitabc --> Output bcmonojita\n\nEpoch 39:: Train Loss 0.1557345688343048\nEpoch 39:: Val Loss 0.14327149093151093\nInput monojitabc --> Output bcmonojita\n\n","output_type":"stream"}]},{"cell_type":"code","source":"test = 'thisabcdif'\npred = am.predict(test)\npred","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:51:19.456939Z","iopub.execute_input":"2024-03-30T15:51:19.457403Z","iopub.status.idle":"2024-03-30T15:51:19.477426Z","shell.execute_reply.started":"2024-03-30T15:51:19.457365Z","shell.execute_reply":"2024-03-30T15:51:19.476346Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"'ifthisabcd'"},"metadata":{}}]},{"cell_type":"code","source":"test_x, test_y = encode_data(x, y, features)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:51:02.110460Z","iopub.status.idle":"2024-03-30T15:51:02.110868Z","shell.execute_reply.started":"2024-03-30T15:51:02.110662Z","shell.execute_reply":"2024-03-30T15:51:02.110678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_loss(one_hot_y, outputs)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:51:02.112692Z","iopub.status.idle":"2024-03-30T15:51:02.113135Z","shell.execute_reply.started":"2024-03-30T15:51:02.112935Z","shell.execute_reply":"2024-03-30T15:51:02.112953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"one_hot_y[:, 10, :]","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:51:02.114064Z","iopub.status.idle":"2024-03-30T15:51:02.114438Z","shell.execute_reply.started":"2024-03-30T15:51:02.114247Z","shell.execute_reply":"2024-03-30T15:51:02.114262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.argmax(one_hot_y[0], dim=-1).shape","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:51:02.116645Z","iopub.status.idle":"2024-03-30T15:51:02.117616Z","shell.execute_reply.started":"2024-03-30T15:51:02.117236Z","shell.execute_reply":"2024-03-30T15:51:02.117263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ce = nn.CrossEntropyLoss()\ntarget_label = torch.argmax(one_hot_y[0], dim=-1)\nprint(outputs[0].shape, target_label.shape)\nce(outputs[0][0], target_label), ce(outputs[0][0], one_hot_y[0])","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:51:02.119645Z","iopub.status.idle":"2024-03-30T15:51:02.120217Z","shell.execute_reply.started":"2024-03-30T15:51:02.119936Z","shell.execute_reply":"2024-03-30T15:51:02.119958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out2[0].repeat(1, 10).shape","metadata":{"execution":{"iopub.status.busy":"2024-03-30T15:51:02.121890Z","iopub.status.idle":"2024-03-30T15:51:02.122279Z","shell.execute_reply.started":"2024-03-30T15:51:02.122096Z","shell.execute_reply":"2024-03-30T15:51:02.122112Z"},"trusted":true},"execution_count":null,"outputs":[]}]}