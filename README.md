# LSTM Attention Mechanism

This is a simple work to understand the working of Attention Mechanism in LSTM. This is a sequence to sequence architecture. 

## Reverse an input sequence
### monojitcnn -> nnctijonom
![Alt Text](https://github.com/MonojitSarkar/attention-lstm-char-manipulation/blob/main/gifs/reverse_animation.gif)

## Left shift an input sequence by 4 characters
### monojitcnn -> jitcnnmono
![Alt Text](https://github.com/MonojitSarkar/attention-lstm-char-manipulation/blob/main/gifs/left_shift_animation.gif)


## Right shift an input sequence by 3 characters
### monojitcnn -> cnnmonojit
![Alt Text](https://github.com/MonojitSarkar/attention-lstm-char-manipulation/blob/main/gifs/right_shift_animation.gif)


## Sort digits in ascending order
### Input [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]
![Alt Text](https://github.com/MonojitSarkar/attention-lstm-char-manipulation/blob/main/gifs/sort_digits_animation.gif)
