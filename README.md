# LSTM Attention Mechanism

This is a simple work to understand the working of Attention Mechanism in LSTM. This is a sequence to sequence architecture. 

## Reverse an input sequence
### monojitcnn -> nnctijonom
![Alt Text](https://github.com/MonojitSarkar/attention-lstm-char-manipulation/blob/main/gifs/reverse_animation.gif)

## Left shift an input sequence by 4 characters
### monojitcnn -> jitcnnmono
![Alt Text](https://github.com/MonojitSarkar/attention-lstm-char-manipulation/blob/main/gifs/left_shift_animation.gif)


## Right shift an input sequence by 3 characters
### monojitcnn -> cnnmonojit
![Alt Text](https://github.com/MonojitSarkar/attention-lstm-char-manipulation/blob/main/gifs/right_shift_animation.gif)
