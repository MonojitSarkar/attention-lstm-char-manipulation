{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport string\nimport numpy as np\nimport random\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nfrom IPython.core.debugger import set_trace\nimport IPython\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-31T10:25:14.481892Z","iopub.execute_input":"2024-03-31T10:25:14.482355Z","iopub.status.idle":"2024-03-31T10:25:18.005364Z","shell.execute_reply.started":"2024-03-31T10:25:14.482318Z","shell.execute_reply":"2024-03-31T10:25:18.003967Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def get_digits(timesteps):\n    rand_indices = [random.randint(0, len(string.digits)-1) for _ in range(timesteps)]\n    rand_nums = list(map(lambda x: int(string.digits[x]), rand_indices))\n    sorted_rand_nums = sorted(rand_nums)\n    \n    return rand_nums, sorted_rand_nums\n\nget_digits(8)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T10:25:18.007907Z","iopub.execute_input":"2024-03-31T10:25:18.008580Z","iopub.status.idle":"2024-03-31T10:25:18.022012Z","shell.execute_reply.started":"2024-03-31T10:25:18.008534Z","shell.execute_reply":"2024-03-31T10:25:18.020554Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"([7, 9, 1, 9, 3, 7, 8, 0], [0, 1, 3, 7, 7, 8, 9, 9])"},"metadata":{}}]},{"cell_type":"code","source":"def generate_training_data(timesteps, num_examples):\n    x = list(); y = list()\n    \n    for _ in range(num_examples):\n        digit, sorted_digit = get_digits(timesteps)\n        x.append(digit); y.append(sorted_digit)\n        \n    random_id = random.randint(0, len(x))\n    print(x[random_id], y[random_id])\n    \n    return x, y","metadata":{"execution":{"iopub.status.busy":"2024-03-31T10:25:18.023837Z","iopub.execute_input":"2024-03-31T10:25:18.024213Z","iopub.status.idle":"2024-03-31T10:25:18.035639Z","shell.execute_reply.started":"2024-03-31T10:25:18.024185Z","shell.execute_reply":"2024-03-31T10:25:18.034559Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def encode_data(x, y):\n    features_dim = 10\n    one_hot_x = F.one_hot(torch.tensor(x), num_classes=features_dim)\n    one_hot_y = F.one_hot(torch.tensor(y), num_classes=features_dim)\n    \n    return one_hot_x.to(torch.float32), one_hot_y.to(torch.float32)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T10:25:18.039281Z","iopub.execute_input":"2024-03-31T10:25:18.039731Z","iopub.status.idle":"2024-03-31T10:25:18.047537Z","shell.execute_reply.started":"2024-03-31T10:25:18.039694Z","shell.execute_reply":"2024-03-31T10:25:18.046695Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"x, y = generate_training_data(8, 10000)\none_hot_x, one_hot_y = encode_data(x, y)\nprint(one_hot_x.dtype, one_hot_y.dtype)\n\nval_x, val_y = generate_training_data(8, 1000)\nval_x, val_y = encode_data(val_x, val_y)\nprint(val_x.shape, val_y.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T10:25:18.048899Z","iopub.execute_input":"2024-03-31T10:25:18.049238Z","iopub.status.idle":"2024-03-31T10:25:18.602784Z","shell.execute_reply.started":"2024-03-31T10:25:18.049207Z","shell.execute_reply":"2024-03-31T10:25:18.601878Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[1, 2, 3, 6, 8, 6, 3, 2] [1, 2, 2, 3, 3, 6, 6, 8]\ntorch.float32 torch.float32\n[1, 5, 0, 5, 6, 6, 5, 7] [0, 1, 5, 5, 5, 6, 6, 7]\ntorch.Size([1000, 8, 10]) torch.Size([1000, 8, 10])\n","output_type":"stream"}]},{"cell_type":"code","source":"class CustomData(Dataset):\n    def __init__(self, x_data, y_data):\n        super().__init__()\n        self.x_data = x_data\n        self.y_data = y_data\n        \n    def __len__(self):\n        return len(self.x_data)\n    \n    def __getitem__(self, idx):\n        return self.x_data[idx], self.y_data[idx]\n    \ndataset = CustomData(one_hot_x, one_hot_y)\nval_dataset = CustomData(val_x, val_y)\n\ntrain_loader = DataLoader(dataset, batch_size=128, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=128, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T10:25:18.604274Z","iopub.execute_input":"2024-03-31T10:25:18.605027Z","iopub.status.idle":"2024-03-31T10:25:18.615283Z","shell.execute_reply.started":"2024-03-31T10:25:18.604985Z","shell.execute_reply":"2024-03-31T10:25:18.613970Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class AttentionModel(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim, input_features_dim, output_features_dim, timesteps, num_training):\n        super().__init__()\n        self.timesteps = timesteps\n        self.input_features_dim = input_features_dim\n        \n        self.densor1 = nn.Linear(decoder_dim+(2*encoder_dim), 10)\n        self.tanh = nn.Tanh()\n        self.densor2 = nn.Linear(10, 1)\n        self.softmax = nn.Softmax(dim=1)\n        \n        self.pre_attention_lstm = nn.LSTM(input_size=input_features_dim, hidden_size=encoder_dim, batch_first=True, bidirectional=True)\n        self.post_attention_lstm = nn.LSTM(input_size=encoder_dim*2, hidden_size=decoder_dim, batch_first=True)\n        self.output_layer = nn.Linear(decoder_dim, output_features_dim)\n        \n        \n    def forward(self, x, s0, c0):\n        # x-> (B, timesteps, features) -> (128, 10, 26)\n        # s0, c0 -> (1, B, decoder_dim) -> (1, 128, 16)\n        #set_trace()\n        self.s = s0; self.c = c0\n        encoder_hidden_states, (hn, cn) = self.pre_attention_lstm(x) #hidden_states: (B, timesteps, encoder_dim*2) -> (128, 10, 16)\n        outputs = list()\n        \n        for t in range(self.timesteps):\n            context = self.one_step_attention(encoder_hidden_states) # (B, timesteps, 2*encoder_dim)\n            _, (self.s, self.c )= self.post_attention_lstm(context, (self.s, self.c)) # _ -> (B, timesteps, decoder_dim)\n            output = self.output_layer(self.s) # (1, B, output_features_dim) -> (1, 128, 26)\n            outputs.append(output) # (timesteps, B, output_features_dim) -> (10, 128, 26)\n        \n        return outputs\n    \n    def one_step_attention(self, encoder_hidden_states):\n        # encoder_Hidden_states -> (B, timesteps, 2*encoder_dim) -> (128, 10, 16)\n        # self.s -> (1, B, decoder_dim)\n        # 1. first the hidden state for decoder must be repeated to match the hidden states of encoder\n        # self.s -> (timesteps, B, decoder_dim) -> permute -> (B, timesteps, decoder_dim)\n        # 2. then concatenate the hidden state for decoder and the hidden state for encoder -> (B, timesteps, 2*encoder_dim + decoder_dim)\n        # pass it through the first dense layer\n        # pass it through the second dense layer\n        # use softmax to decide which hidden state of encoder is the most important\n        # use dot product to find the important hidden state of encoder and feed it as input to the decoder\n        hidden_decoder = self.s.repeat(self.timesteps, 1, 1).permute(1, 0, 2) # (B, timesteps, decoder_dim) -> (128, 10, 16)\n        concat = torch.concatenate([encoder_hidden_states, hidden_decoder], dim=-1) # (B, timesteps, 2*encoder_dim + decoder_dim) -> (128, 10, 32)\n        e = self.tanh(self.densor1(concat)) # (B, timesteps, 10) -> (128, 10, 10)\n        energies = self.softmax(self.densor2(e)) # (B, timesteps, 1) -> (128, 10, 1)\n        # let's if without permute in next step if the code converges\n        energies = energies.repeat(1, 1, 10).permute(0, 2, 1) # (B, timesteps, 10) -> (B, 10, 10)\n        context = torch.bmm(energies, encoder_hidden_states) # (B, timesteps, timesteps) @ (B, timesteps, 2*encoder_dim) -> (B, timesteps, 2*encoder_dim)\n        \n        return context\n        \n    def predict(self, x):\n        assert len(x) == self.timesteps\n        one_hot = F.one_hot(torch.tensor(x), num_classes=self.input_features_dim)\n        s0 = torch.zeros(1, 1, decoder_dim); c0 = torch.zeros(1, 1, decoder_dim)\n        one_hot = one_hot.unsqueeze(0)\n        pred = self.forward(one_hot.to(torch.float32), s0, c0)\n        pred = [torch.argmax(t, dim=-1).tolist()[0][0] for t in pred]\n        return pred","metadata":{"execution":{"iopub.status.busy":"2024-03-31T10:25:18.617363Z","iopub.execute_input":"2024-03-31T10:25:18.617873Z","iopub.status.idle":"2024-03-31T10:25:18.637394Z","shell.execute_reply.started":"2024-03-31T10:25:18.617834Z","shell.execute_reply":"2024-03-31T10:25:18.636458Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def calculate_loss(y_true, y_pred):\n    ce = nn.CrossEntropyLoss()\n    total = 0\n    for target, logit in zip(list(y_true), list(y_pred)):\n        loss = ce(logit[0], target)\n        total += loss\n        \n    return total","metadata":{"execution":{"iopub.status.busy":"2024-03-31T10:25:18.638595Z","iopub.execute_input":"2024-03-31T10:25:18.639781Z","iopub.status.idle":"2024-03-31T10:25:18.654149Z","shell.execute_reply.started":"2024-03-31T10:25:18.639746Z","shell.execute_reply":"2024-03-31T10:25:18.653063Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"imp = list()\n\ndef hook_function(module, input, output):\n    if not am.training:\n        imp.append(output[0].flatten().tolist())","metadata":{"execution":{"iopub.status.busy":"2024-03-31T10:46:06.960967Z","iopub.execute_input":"2024-03-31T10:46:06.961389Z","iopub.status.idle":"2024-03-31T10:46:06.968287Z","shell.execute_reply.started":"2024-03-31T10:46:06.961361Z","shell.execute_reply":"2024-03-31T10:46:06.966888Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"features = 10\nencoder_dim = 8\ndecoder_dim = 16\ninput_features_dim = features\noutput_features_dim = features\ntimesteps = 8\nnum_training = 10000\nepochs = 40\n\nattention_dict = dict()\n\ns0, c0 = torch.zeros(1, num_training, decoder_dim), torch.zeros(1, num_training, decoder_dim)\n\nam = AttentionModel(encoder_dim, decoder_dim, input_features_dim, output_features_dim, timesteps, num_training)\nopt = optim.Adam(am.parameters())","metadata":{"execution":{"iopub.status.busy":"2024-03-31T10:46:07.810320Z","iopub.execute_input":"2024-03-31T10:46:07.810781Z","iopub.status.idle":"2024-03-31T10:46:07.822484Z","shell.execute_reply.started":"2024-03-31T10:46:07.810750Z","shell.execute_reply":"2024-03-31T10:46:07.821230Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"for name, layer in am.named_children():\n    if name == 'softmax':\n        print(layer)\n        handle = layer.register_forward_hook(hook_function)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T10:46:09.270558Z","iopub.execute_input":"2024-03-31T10:46:09.270969Z","iopub.status.idle":"2024-03-31T10:46:09.278601Z","shell.execute_reply.started":"2024-03-31T10:46:09.270940Z","shell.execute_reply":"2024-03-31T10:46:09.277161Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Softmax(dim=1)\n","output_type":"stream"}]},{"cell_type":"code","source":"for epoch in range(epochs):\n    am.train()\n    train_loss = list()\n    for i, (inputs, target) in enumerate(train_loader):\n        #set_trace()\n        target = target.permute(1, 0, 2)\n        batch_size = inputs.size(0)\n        s0 = torch.zeros(1, batch_size, decoder_dim); c0 = torch.zeros(1, batch_size, decoder_dim)\n        outputs = am(inputs, s0, c0)\n        \n        opt.zero_grad()\n        \n        total_loss = calculate_loss(target, outputs)\n        train_loss.append(total_loss)\n        \n        total_loss.backward()\n        opt.step()\n        \n    print(f'Epoch {epoch}:: Train Loss {torch.mean(torch.tensor(train_loss))}')\n    \n    am.eval()\n    with torch.no_grad():\n        \n#         s0 = torch.zeros(1, 1000, decoder_dim); c0 = torch.zeros(1, 1000, decoder_dim)\n#         out = am(val_x, s0, c0)\n        \n#         val_loss = calculate_loss(val_y.permute(1, 0, 2), out)\n        \n#         print(f'Epoch {epoch}:: Val Loss {val_loss}')\n        test = [1, 9, 4, 3, 8, 7, 5, 6]\n        #set_trace()\n        pred = am.predict(test)\n        attention_dict[f'Epoch {epoch}'] = np.array(imp)\n        imp = list()\n        print(f'Input {test} --> Output {pred}')\n        print()\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-31T10:46:10.421590Z","iopub.execute_input":"2024-03-31T10:46:10.422004Z","iopub.status.idle":"2024-03-31T10:48:05.359941Z","shell.execute_reply.started":"2024-03-31T10:46:10.421976Z","shell.execute_reply":"2024-03-31T10:48:05.358804Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Epoch 0:: Train Loss 18.33650016784668\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [7, 7, 7, 7, 7, 7, 7, 7]\n\nEpoch 1:: Train Loss 15.879549026489258\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [0, 3, 3, 5, 7, 8, 8, 8]\n\nEpoch 2:: Train Loss 11.252341270446777\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 8, 8, 9]\n\nEpoch 3:: Train Loss 8.750053405761719\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 4:: Train Loss 7.315134525299072\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 5:: Train Loss 6.429124355316162\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 6:: Train Loss 5.753997802734375\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 7:: Train Loss 5.24852991104126\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 8:: Train Loss 4.822076320648193\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 9:: Train Loss 4.530144691467285\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 8]\n\nEpoch 10:: Train Loss 4.274902820587158\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 11:: Train Loss 3.9824182987213135\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 12:: Train Loss 3.766435146331787\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 13:: Train Loss 3.5803370475769043\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 14:: Train Loss 3.3998749256134033\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 15:: Train Loss 3.249943256378174\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 16:: Train Loss 3.1162943840026855\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 17:: Train Loss 2.98504638671875\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 18:: Train Loss 2.8910210132598877\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 19:: Train Loss 2.8224616050720215\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 20:: Train Loss 2.6976089477539062\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 21:: Train Loss 2.6177306175231934\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 22:: Train Loss 2.566563367843628\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 23:: Train Loss 2.4730281829833984\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 24:: Train Loss 2.368553638458252\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 25:: Train Loss 2.3247148990631104\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 26:: Train Loss 2.2436697483062744\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 27:: Train Loss 2.176438808441162\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 28:: Train Loss 2.029303550720215\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 29:: Train Loss 1.9410253763198853\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 30:: Train Loss 1.8841310739517212\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 31:: Train Loss 1.7399778366088867\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 32:: Train Loss 1.6599117517471313\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 33:: Train Loss 1.5575356483459473\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 34:: Train Loss 1.467502236366272\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 35:: Train Loss 1.3479561805725098\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 36:: Train Loss 1.2138923406600952\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 37:: Train Loss 1.154783844947815\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 38:: Train Loss 1.022247314453125\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 39:: Train Loss 0.9063401818275452\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"test1 = [8, 8, 1, 7, 5, 4, 3, 3]; test2 = [8, 0, 1, 3, 2, 2, 9, 3]\nheat_dict = dict()\nfor i, test in enumerate([test1, test2]):\n    pred = am.predict(test)\n    print(pred)\n    print(len(imp))\n    heat_dict[f'Test {i}'] = np.array(imp)\n    imp = list()\n    print(f'After adding to dictionary {len(imp)}')","metadata":{"execution":{"iopub.status.busy":"2024-03-31T10:32:39.240923Z","iopub.execute_input":"2024-03-31T10:32:39.241404Z","iopub.status.idle":"2024-03-31T10:32:39.282325Z","shell.execute_reply.started":"2024-03-31T10:32:39.241372Z","shell.execute_reply":"2024-03-31T10:32:39.281221Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[1, 3, 3, 4, 5, 7, 8, 8]\n8\nAfter adding to dictionary 0\n[0, 1, 2, 2, 3, 3, 8, 9]\n8\nAfter adding to dictionary 0\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = list()\nfor key in attention_dict:\n    plt.imshow(attention_dict[key])\n    plt.xticks(range(len(test)), labels=test)\n    plt.yticks(range(len(test)), labels=test)\n    \n    fig = plt.gcf()\n    fig.canvas.draw()\n    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n    image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n    plt.clf()\n    \n    images.append(image)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T10:57:45.131216Z","iopub.execute_input":"2024-03-31T10:57:45.131672Z","iopub.status.idle":"2024-03-31T10:57:49.955970Z","shell.execute_reply.started":"2024-03-31T10:57:45.131642Z","shell.execute_reply":"2024-03-31T10:57:49.955063Z"},"trusted":true},"execution_count":48,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}}]},{"cell_type":"code","source":"import imageio\n\n# images = list(attention_dict.values())\nimageio.mimsave('/kaggle/working/plots.gif', images, duration=10)","metadata":{"execution":{"iopub.status.busy":"2024-03-31T11:00:37.181471Z","iopub.execute_input":"2024-03-31T11:00:37.181890Z","iopub.status.idle":"2024-03-31T11:00:38.804081Z","shell.execute_reply.started":"2024-03-31T11:00:37.181860Z","shell.execute_reply":"2024-03-31T11:00:38.802963Z"},"trusted":true},"execution_count":52,"outputs":[]}]}