{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport string\nimport numpy as np\nimport random\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nfrom IPython.core.debugger import set_trace\nimport IPython\nimport matplotlib.pyplot as plt\nfrom PIL import Image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-01T17:28:40.111932Z","iopub.execute_input":"2024-04-01T17:28:40.112338Z","iopub.status.idle":"2024-04-01T17:28:40.120152Z","shell.execute_reply.started":"2024-04-01T17:28:40.112306Z","shell.execute_reply":"2024-04-01T17:28:40.118678Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def get_digits(timesteps):\n    rand_indices = [random.randint(0, len(string.digits)-1) for _ in range(timesteps)]\n    rand_nums = list(map(lambda x: int(string.digits[x]), rand_indices))\n    sorted_rand_nums = sorted(rand_nums)\n    \n    return rand_nums, sorted_rand_nums\n\nget_digits(8)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:24:13.662536Z","iopub.execute_input":"2024-04-01T17:24:13.663348Z","iopub.status.idle":"2024-04-01T17:24:13.674264Z","shell.execute_reply.started":"2024-04-01T17:24:13.663312Z","shell.execute_reply":"2024-04-01T17:24:13.673063Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"([4, 1, 2, 5, 1, 5, 3, 1], [1, 1, 1, 2, 3, 4, 5, 5])"},"metadata":{}}]},{"cell_type":"code","source":"def generate_training_data(timesteps, num_examples):\n    x = list(); y = list()\n    \n    for _ in range(num_examples):\n        digit, sorted_digit = get_digits(timesteps)\n        x.append(digit); y.append(sorted_digit)\n        \n    random_id = random.randint(0, len(x))\n    print(x[random_id], y[random_id])\n    \n    return x, y","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:24:16.749525Z","iopub.execute_input":"2024-04-01T17:24:16.751014Z","iopub.status.idle":"2024-04-01T17:24:16.759446Z","shell.execute_reply.started":"2024-04-01T17:24:16.750944Z","shell.execute_reply":"2024-04-01T17:24:16.757827Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def encode_data(x, y):\n    features_dim = 10\n    one_hot_x = F.one_hot(torch.tensor(x), num_classes=features_dim)\n    one_hot_y = F.one_hot(torch.tensor(y), num_classes=features_dim)\n    \n    return one_hot_x.to(torch.float32), one_hot_y.to(torch.float32)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:24:17.426634Z","iopub.execute_input":"2024-04-01T17:24:17.430237Z","iopub.status.idle":"2024-04-01T17:24:17.439322Z","shell.execute_reply.started":"2024-04-01T17:24:17.430188Z","shell.execute_reply":"2024-04-01T17:24:17.437723Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"x, y = generate_training_data(8, 10000)\none_hot_x, one_hot_y = encode_data(x, y)\nprint(one_hot_x.dtype, one_hot_y.dtype)\n\nval_x, val_y = generate_training_data(8, 1000)\nval_x, val_y = encode_data(val_x, val_y)\nprint(val_x.shape, val_y.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:24:18.077869Z","iopub.execute_input":"2024-04-01T17:24:18.078941Z","iopub.status.idle":"2024-04-01T17:24:18.543971Z","shell.execute_reply.started":"2024-04-01T17:24:18.078890Z","shell.execute_reply":"2024-04-01T17:24:18.542077Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[9, 1, 1, 1, 7, 4, 1, 9] [1, 1, 1, 1, 4, 7, 9, 9]\ntorch.float32 torch.float32\n[8, 0, 2, 5, 4, 5, 9, 1] [0, 1, 2, 4, 5, 5, 8, 9]\ntorch.Size([1000, 8, 10]) torch.Size([1000, 8, 10])\n","output_type":"stream"}]},{"cell_type":"code","source":"class CustomData(Dataset):\n    def __init__(self, x_data, y_data):\n        super().__init__()\n        self.x_data = x_data\n        self.y_data = y_data\n        \n    def __len__(self):\n        return len(self.x_data)\n    \n    def __getitem__(self, idx):\n        return self.x_data[idx], self.y_data[idx]\n    \ndataset = CustomData(one_hot_x, one_hot_y)\nval_dataset = CustomData(val_x, val_y)\n\ntrain_loader = DataLoader(dataset, batch_size=128, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=128, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:24:19.029980Z","iopub.execute_input":"2024-04-01T17:24:19.030415Z","iopub.status.idle":"2024-04-01T17:24:19.042519Z","shell.execute_reply.started":"2024-04-01T17:24:19.030379Z","shell.execute_reply":"2024-04-01T17:24:19.041239Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class AttentionModel(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim, input_features_dim, output_features_dim, timesteps, num_training):\n        super().__init__()\n        self.timesteps = timesteps\n        self.input_features_dim = input_features_dim\n        \n        self.densor1 = nn.Linear(decoder_dim+(2*encoder_dim), 10)\n        self.tanh = nn.Tanh()\n        self.densor2 = nn.Linear(10, 1)\n        self.softmax = nn.Softmax(dim=1)\n        \n        self.pre_attention_lstm = nn.LSTM(input_size=input_features_dim, hidden_size=encoder_dim, batch_first=True, bidirectional=True)\n        self.post_attention_lstm = nn.LSTM(input_size=encoder_dim*2, hidden_size=decoder_dim, batch_first=True)\n        self.output_layer = nn.Linear(decoder_dim, output_features_dim)\n        \n        \n    def forward(self, x, s0, c0):\n        # x-> (B, timesteps, features) -> (128, 10, 26)\n        # s0, c0 -> (1, B, decoder_dim) -> (1, 128, 16)\n        #set_trace()\n        self.s = s0; self.c = c0\n        encoder_hidden_states, (hn, cn) = self.pre_attention_lstm(x) #hidden_states: (B, timesteps, encoder_dim*2) -> (128, 10, 16)\n        outputs = list()\n        \n        for t in range(self.timesteps):\n            context = self.one_step_attention(encoder_hidden_states) # (B, timesteps, 2*encoder_dim)\n            _, (self.s, self.c )= self.post_attention_lstm(context, (self.s, self.c)) # _ -> (B, timesteps, decoder_dim)\n            output = self.output_layer(self.s) # (1, B, output_features_dim) -> (1, 128, 26)\n            outputs.append(output) # (timesteps, B, output_features_dim) -> (10, 128, 26)\n        \n        return outputs\n    \n    def one_step_attention(self, encoder_hidden_states):\n        # encoder_Hidden_states -> (B, timesteps, 2*encoder_dim) -> (128, 10, 16)\n        # self.s -> (1, B, decoder_dim)\n        # 1. first the hidden state for decoder must be repeated to match the hidden states of encoder\n        # self.s -> (timesteps, B, decoder_dim) -> permute -> (B, timesteps, decoder_dim)\n        # 2. then concatenate the hidden state for decoder and the hidden state for encoder -> (B, timesteps, 2*encoder_dim + decoder_dim)\n        # pass it through the first dense layer\n        # pass it through the second dense layer\n        # use softmax to decide which hidden state of encoder is the most important\n        # use dot product to find the important hidden state of encoder and feed it as input to the decoder\n        hidden_decoder = self.s.repeat(self.timesteps, 1, 1).permute(1, 0, 2) # (B, timesteps, decoder_dim) -> (128, 10, 16)\n        concat = torch.concatenate([encoder_hidden_states, hidden_decoder], dim=-1) # (B, timesteps, 2*encoder_dim + decoder_dim) -> (128, 10, 32)\n        e = self.tanh(self.densor1(concat)) # (B, timesteps, 10) -> (128, 10, 10)\n        energies = self.softmax(self.densor2(e)) # (B, timesteps, 1) -> (128, 10, 1)\n        # let's if without permute in next step if the code converges\n        energies = energies.repeat(1, 1, 10).permute(0, 2, 1) # (B, timesteps, 10) -> (B, 10, 10)\n        context = torch.bmm(energies, encoder_hidden_states) # (B, timesteps, timesteps) @ (B, timesteps, 2*encoder_dim) -> (B, timesteps, 2*encoder_dim)\n        \n        return context\n        \n    def predict(self, x):\n        assert len(x) == self.timesteps\n        one_hot = F.one_hot(torch.tensor(x), num_classes=self.input_features_dim)\n        s0 = torch.zeros(1, 1, decoder_dim); c0 = torch.zeros(1, 1, decoder_dim)\n        one_hot = one_hot.unsqueeze(0)\n        pred = self.forward(one_hot.to(torch.float32), s0, c0)\n        pred = [torch.argmax(t, dim=-1).tolist()[0][0] for t in pred]\n        return pred","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:24:19.859159Z","iopub.execute_input":"2024-04-01T17:24:19.859613Z","iopub.status.idle":"2024-04-01T17:24:19.882165Z","shell.execute_reply.started":"2024-04-01T17:24:19.859568Z","shell.execute_reply":"2024-04-01T17:24:19.880569Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def calculate_loss(y_true, y_pred):\n    ce = nn.CrossEntropyLoss()\n    total = 0\n    for target, logit in zip(list(y_true), list(y_pred)):\n        loss = ce(logit[0], target)\n        total += loss\n        \n    return total","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:24:20.425341Z","iopub.execute_input":"2024-04-01T17:24:20.425785Z","iopub.status.idle":"2024-04-01T17:24:20.432748Z","shell.execute_reply.started":"2024-04-01T17:24:20.425753Z","shell.execute_reply":"2024-04-01T17:24:20.431343Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"imp = list()\n\ndef hook_function(module, input, output):\n    if not am.training:\n        imp.append(output[0].flatten().tolist())","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:24:21.330782Z","iopub.execute_input":"2024-04-01T17:24:21.331837Z","iopub.status.idle":"2024-04-01T17:24:21.339082Z","shell.execute_reply.started":"2024-04-01T17:24:21.331793Z","shell.execute_reply":"2024-04-01T17:24:21.337385Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"features = 10\nencoder_dim = 8\ndecoder_dim = 16\ninput_features_dim = features\noutput_features_dim = features\ntimesteps = 8\nnum_training = 10000\nepochs = 40\n\nattention_dict = dict()\n\ns0, c0 = torch.zeros(1, num_training, decoder_dim), torch.zeros(1, num_training, decoder_dim)\n\nam = AttentionModel(encoder_dim, decoder_dim, input_features_dim, output_features_dim, timesteps, num_training)\nopt = optim.Adam(am.parameters())","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:24:21.925392Z","iopub.execute_input":"2024-04-01T17:24:21.925782Z","iopub.status.idle":"2024-04-01T17:24:23.768776Z","shell.execute_reply.started":"2024-04-01T17:24:21.925754Z","shell.execute_reply":"2024-04-01T17:24:23.767252Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"for name, layer in am.named_children():\n    if name == 'softmax':\n        print(layer)\n        handle = layer.register_forward_hook(hook_function)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:24:23.771865Z","iopub.execute_input":"2024-04-01T17:24:23.772555Z","iopub.status.idle":"2024-04-01T17:24:23.781270Z","shell.execute_reply.started":"2024-04-01T17:24:23.772506Z","shell.execute_reply":"2024-04-01T17:24:23.779572Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Softmax(dim=1)\n","output_type":"stream"}]},{"cell_type":"code","source":"for epoch in range(epochs):\n    am.train()\n    train_loss = list()\n    for i, (inputs, target) in enumerate(train_loader):\n        #set_trace()\n        target = target.permute(1, 0, 2)\n        batch_size = inputs.size(0)\n        s0 = torch.zeros(1, batch_size, decoder_dim); c0 = torch.zeros(1, batch_size, decoder_dim)\n        outputs = am(inputs, s0, c0)\n        \n        opt.zero_grad()\n        \n        total_loss = calculate_loss(target, outputs)\n        train_loss.append(total_loss)\n        \n        total_loss.backward()\n        opt.step()\n        \n    print(f'Epoch {epoch}:: Train Loss {torch.mean(torch.tensor(train_loss))}')\n    \n    am.eval()\n    with torch.no_grad():\n        \n#         s0 = torch.zeros(1, 1000, decoder_dim); c0 = torch.zeros(1, 1000, decoder_dim)\n#         out = am(val_x, s0, c0)\n        \n#         val_loss = calculate_loss(val_y.permute(1, 0, 2), out)\n        \n#         print(f'Epoch {epoch}:: Val Loss {val_loss}')\n        test = [1, 9, 4, 3, 8, 7, 5, 6]\n        #set_trace()\n        pred = am.predict(test)\n        attention_dict[f'Epoch {epoch}'] = np.array(imp)\n        imp = list()\n        print(f'Input {test} --> Output {pred}')\n        print()\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:24:23.783289Z","iopub.execute_input":"2024-04-01T17:24:23.783759Z","iopub.status.idle":"2024-04-01T17:26:06.698639Z","shell.execute_reply.started":"2024-04-01T17:24:23.783716Z","shell.execute_reply":"2024-04-01T17:26:06.696529Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Epoch 0:: Train Loss 18.358272552490234\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [8, 8, 1, 1, 1, 1, 1, 1]\n\nEpoch 1:: Train Loss 16.833295822143555\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [0, 3, 3, 8, 9, 9, 9, 9]\n\nEpoch 2:: Train Loss 12.472617149353027\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [0, 3, 3, 5, 6, 7, 9, 9]\n\nEpoch 3:: Train Loss 9.79032039642334\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [0, 3, 3, 5, 6, 7, 9, 9]\n\nEpoch 4:: Train Loss 8.18410587310791\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [0, 3, 3, 5, 6, 7, 9, 9]\n\nEpoch 5:: Train Loss 7.161725997924805\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 9, 9]\n\nEpoch 6:: Train Loss 6.386471271514893\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 7:: Train Loss 5.746674537658691\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 8:: Train Loss 5.297490119934082\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 9:: Train Loss 4.887543201446533\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 10:: Train Loss 4.566585063934326\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 11:: Train Loss 4.270510673522949\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 12:: Train Loss 4.071699142456055\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [2, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 13:: Train Loss 3.8260598182678223\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 14:: Train Loss 3.6582155227661133\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 15:: Train Loss 3.491253137588501\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 16:: Train Loss 3.3287360668182373\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [2, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 17:: Train Loss 3.153615713119507\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 18:: Train Loss 3.005321741104126\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 19:: Train Loss 2.8473706245422363\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 20:: Train Loss 2.66202712059021\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [2, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 21:: Train Loss 2.5118308067321777\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 22:: Train Loss 2.356837511062622\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [2, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 23:: Train Loss 2.2283594608306885\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 24:: Train Loss 2.0802927017211914\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 25:: Train Loss 1.9301679134368896\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 26:: Train Loss 1.7789851427078247\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 27:: Train Loss 1.656474232673645\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 28:: Train Loss 1.5307157039642334\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 29:: Train Loss 1.433477520942688\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 30:: Train Loss 1.349985957145691\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 31:: Train Loss 1.2796765565872192\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 32:: Train Loss 1.2413047552108765\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 33:: Train Loss 1.1586389541625977\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 34:: Train Loss 1.1217979192733765\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 35:: Train Loss 1.0865399837493896\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 36:: Train Loss 1.0623332262039185\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 37:: Train Loss 1.0165989398956299\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 38:: Train Loss 0.9805545210838318\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\nEpoch 39:: Train Loss 0.9199075698852539\nInput [1, 9, 4, 3, 8, 7, 5, 6] --> Output [1, 3, 4, 5, 6, 7, 8, 9]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.imshow(attention_dict['Epoch 39'], cmap='plasma')","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:27:36.312562Z","iopub.execute_input":"2024-04-01T17:27:36.313235Z","iopub.status.idle":"2024-04-01T17:27:36.681686Z","shell.execute_reply.started":"2024-04-01T17:27:36.313197Z","shell.execute_reply":"2024-04-01T17:27:36.680266Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7d4b3f6dad40>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"for key in attention_dict:\n    plt.imshow(attention_dict[key], cmap='plasma')\n    plt.title(key)\n    plt.title(f\"Attention Heatmap sort digits \\n {key}\")\n    plt.xticks(range(len(test)), labels=test)\n    plt.yticks(range(len(test)), labels=test)\n    plt.colorbar()\n    \n    file_name = f'{key}.png'\n    plt.savefig(f'/kaggle/working/{file_name}')\n    plt.close()","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:28:13.492283Z","iopub.execute_input":"2024-04-01T17:28:13.492736Z","iopub.status.idle":"2024-04-01T17:28:26.231419Z","shell.execute_reply.started":"2024-04-01T17:28:13.492700Z","shell.execute_reply":"2024-04-01T17:28:26.229944Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"frames = list()\n\nfor key in attention_dict:\n    img = Image.open(f'/kaggle/working/{key}.png')\n    img = img.convert('RGB')\n    \n    frames.append(img)\n    \nframes[0].save('sort_digits_animation.gif', save_all=True, append_images=frames[1:], duration=150, loop=0)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:28:45.242154Z","iopub.execute_input":"2024-04-01T17:28:45.242568Z","iopub.status.idle":"2024-04-01T17:28:47.418230Z","shell.execute_reply.started":"2024-04-01T17:28:45.242538Z","shell.execute_reply":"2024-04-01T17:28:47.416984Z"},"trusted":true},"execution_count":18,"outputs":[]}]}